Building a Character Animation System

1.0.	      Introduction
Animating virtual humans is a complex task. Many diﬀerent aspects of human behavior need to be modeled in order to generate a convincing result. The behavior and appearance of a virtual character needs to be recognizably human in expression, although photorealism is not necessary. 
People are adept at recognizing movement and human-like behavior, so the actions and appearance of a virtual character must match the expectations of the human viewer. This means that not only must the character’s movements be natural, but they must be contextually appropriate, such as responding with the appropriate reaction and in the proper time frame to stimuli. 

1.1.      Background
Research has been done on various aspects of character animation, such as locomotion and facial animation. However, the integration of all these aspects leads to complexities. For example, coordinating locomotion with path ﬁnding, or coordinating reaching with gazing. 
At ﬁrst glance, it appears that modeling an entire animated character can be achieved by combining individual areas, and then reassembling the ﬁnal character as a combination of each individual part. For example, locomotion can be combined with a lip sync animation. This combination works since there is little relationship between locomotion and the movement of a character’s lips and face. Serious problems arrive when the areas overlap and directly or indirectly impact each other. For example, if you were to perform a manipulation with your hands while simultaneously looking at another object in the virtual world. The looking behavior might engage parts of the character’s body that disrupt the manipulation. Thus, although manipulation and gazing are distinct problem areas in animation research, they can interact with each other in unexpected ways.

1.2.    Problem Statement
This project will address the challenge of simulating the different aspects of human behaviour and more importantly the difficulties that arise as we try to integrate all these aspects. This is what we need if we are to synthesize a highly realistic, interactive character. 
Many game engines provide robust solutions to many real time simulation problems such as mesh rendering, lighting, particle eﬀects and so forth. However, game engines generally do not handle complex character animation.

1.3. Objectives

1.3.1. Main objective.
The main goal of this project is to develop a character animation system that allows the realization of common human behaviors that are used in real time games and simulations. 
These behaviors include: synthesizing speech, responding to speech, moving, touching, grabbing, gesturing, and gazing, breathing, emotional expression and other non-verbal behaviour.

1.3.2. Specific Objectives
•	To build a simulation model.
•	To build a prototype of our simulation model.
•	To implement the verbal behaviours using BML as the interface.
•	To locate and implement example based systems for the non-verbal behaviours.
•	To test and validate the system.

1.4. Scope
This system is for animating virtual characters and it encompasses many important aspects of character modeling for simulations and games. These aspects include locomotion, facial animation, speech synthesis, reaching/grabbing, and various automated non-verbal behaviors, such as nodding, gesturing and eye saccades.
1.5. Research Significance
This study is important because it looks at implementing the various aspects of character animation, which if integrated well, yield high levels of realism and control.
2.0. Literature review
We looked at a number of different character animation systems including SmartBody. Most of them use a hierarchical, controller-based architecture. The state of the character is manipulated by series of controllers, with the output of one passed as the input to another. Each controller can override, modify or ignore the state of the virtual character. The SmartBody system uses the Behavioral Markup Language (BML) as an interface for controlling and synchronization speech, gesturing and other aspects of conversational modeling. 
Designing a character animation system requires the development of a number of diﬀerent capabilities for a virtual character. However, the decision to use those capabilities is up to the simulation designer, game designer, or agent (AI) developer. The way most systems approached this was not effective.
Our solution to this problem is to employ a separate component that handles non-verbal behavior such as gesturing, head nodding, idle gaze behavior and facial expression. This component enhances or changes instructions from the agent before sending them to the motion engine. Thus, a complex motion that engages many diﬀerent behaviors at once, such as head nodding while blinking, speaking and emoting, can be hidden from the agent designer.

2.0.	 Methodology
The proposed methodology consists of two distinct phases: The model development phase and the model/animation run-time phase. In the first of the two phases the simulation model is built and tested and the prototype implementation can be done in any modeling/simulation system. Furthermore the animation model is built and tested; this could also be done in several environments. 
The system will be written almost entirely in C++, and will be ported to run on Linux, OsX and Windows. In a later date, we will also be able to port to the mobile platforms, Android and iOs.
We will use BML as an interface for controlling and synchronization speech, gesturing and other aspects of conversational modeling.
We shall implement the non-verbal behaviors such as locomotion and path finding using example based systems which we will also improve as required.


References
1. Ari Shapiro.: Building a character animation system. Institute for Creative Technologies. In: Proceedings of the 4th international conference on Motion in Games Pages 98-109. (November 2011), http://dl.acm.org/citation.cfm?id=2177830
2. Lee, J., Wang, Z., Marsella, S.: Evaluating models of speaker head nods for virtual agents. In: Proceedings of the 9th International Conference on Autonomous Agents and Multiagent Systems: volume 1 - Volume 1. pp. 1257–1264. AAMAS ’10, Inter- national Foundation for Autonomous Agents and Multiagent Systems, Richland, SC (2010), http://dl.acm.org/citation.cfm?id=1838206.1838370
3. Johansen, R.S.: Automated Semi-Procedural Animation for Character Locomotion. Master’s thesis, Aarhus University, the Netherlands (2009)


